{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47dd5db-5f63-42ed-8a7d-1cc655d39dcf",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "ans->Min-Max scaling is a data preprocessing technique used to scale numerical features in a dataset to a specific range, typically between 0 and 1. It's useful when the features in your dataset have varying scales, and you want to ensure that they all have the same scale for machine learning algorithms that are sensitive to the magnitude of features.\n",
    "\n",
    "x'= (x - x_min) / (x_max - x_min)\n",
    "\n",
    "Suppose you have a dataset of ages as follows:\n",
    "\n",
    "Person 1: Age = 25(min)\n",
    "Person 2: Age = 45(max)\n",
    "Person 3: Age = 30\n",
    "\n",
    "After Min-Max scaling, your dataset will look like this:\n",
    "\n",
    "Person 1: Age = 0\n",
    "Person 2: Age = 1\n",
    "Person 3: Age = 0.5\n",
    "Now, all the ages are within the range of 0 to 1, making it easier to compare and analyze these values, especially when using machine learning algorithms that are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd39c5f-72f4-4648-9cf1-a84bf169889e",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "ans->The Unit Vector technique, also known as \"Normalization\" or \"L2 Normalization,\" is a feature scaling method that scales the values of numerical features to have a unit norm or magnitude of 1. Unlike Min-Max scaling, which scales values to a specific range (e.g., 0 to 1), Unit Vector scaling focuses on the direction or orientation of the data points in a multidimensional space rather than their magnitude.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with two numerical features, 'Feature1' and 'Feature2,' and you want to normalize them using Unit Vector scaling:\n",
    "\n",
    "Data point 1: (Feature1 = 3, Feature2 = 4)\n",
    "Data point 2: (Feature1 = 1, Feature2 = 2)\n",
    "Calculate the L2 norm for each data point:\n",
    "\n",
    "For Data point 1: √(9+16) = 5\n",
    "For Data point 2: √(1+4) = √5\n",
    "\n",
    "Normalize each data point by dividing by its L2 norm:\n",
    "\n",
    "Normalized Data point 1:  ( (3/5),(4/5) )\n",
    "\n",
    "Normalized Data point 2:  ( (1/V5),(2/V5) )\n",
    "\n",
    "In this case, both normalized data points have a magnitude (L2 norm) of 1. This scaling method is useful when the direction of the data points in high-dimensional space matters more than their absolute values. It's commonly used in machine learning algorithms that rely on distance measures, such as k-nearest neighbors (KNN) and support vector machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81404ff9-371f-4dcc-8731-241423b5eaf0",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "ans->PCA (Principal Component Analysis) is a dimensionality reduction technique used in data analysis and machine learning to transform high-dimensional data into a lower-dimensional representation while retaining as much of the original variance as possible. It achieves this by identifying and selecting a set of new orthogonal axes called principal components, which capture the most significant information in the data.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "Standardize the data: If the features have different scales, it's essential to standardize them (mean = 0, standard deviation = 1) to give each feature equal importance in the analysis.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance between each pair of standardized features. The covariance matrix describes the relationships between features.\n",
    "\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix: The eigenvectors represent the principal components, and the eigenvalues indicate their importance. The principal components are sorted in descending order of their corresponding eigenvalues.\n",
    "\n",
    "Select a subset of the principal components: Choose the top  k principal components that capture most of the variance in the data. Typically, you decide on the number of components based on how much variance you want to retain (e.g., 95% of the total variance).\n",
    "\n",
    "Project the data onto the selected principal components: Transform the original data into the lower-dimensional space defined by the selected principal components.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with three features: 'Height,' 'Weight,' and 'Age.' You want to reduce the dimensionality of this dataset using PCA. After standardizing the data and computing the covariance matrix, you find the following eigenvalues and eigenvectors:\n",
    "\n",
    "Eigenvalues:λ1=2.5,λ2=1.2,λ3=0.3\n",
    "\n",
    "\n",
    "Eigenvector: PC1=[0.6,0.7,0.4], PC2=[−0.7,0.6,0.1],PC3=[0.3,0.3,−0.9]\n",
    "\n",
    "You decide to retain the top two principal components, which capture 2.5 + 1.2 = 3.7 units of variance, representing a high proportion of the total variance. You project the data onto these two components:\n",
    "\n",
    "Data point 1: [2, 150, 30] -> Projected to [2.1, 1.7]\n",
    "Data point 2: [1.8, 160, 28] -> Projected to [2.0, 1.8]\n",
    "...\n",
    "Now, your data is in a lower-dimensional space with reduced complexity, making it easier to analyze and visualize while preserving the most important information.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c44282-7268-4903-840d-a258cc7474e5",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "ans->PCA is a technique for feature extraction, which involves reducing the dimensionality of a dataset while preserving essential information. It identifies and selects a subset of new features called principal components, based on their ability to capture the most significant variance in the original data. These principal components can be used as the new, lower-dimensional features in data analysis and machine learning.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with 10 numerical features, and you want to perform feature extraction using PCA. After standardizing the data, PCA reveals that the first three principal components capture the most significant variance.\n",
    "\n",
    "You decide to retain these three components. Each data point in your dataset is then projected onto this lower-dimensional space defined by the three principal components, effectively reducing the data from 10 features to 3. These three components can be considered as the new features extracted from the original dataset.\n",
    "\n",
    "By using PCA for feature extraction, you've reduced the dimensionality of your data while retaining the most important information, which can be helpful for improving the efficiency and interpretability of machine learning models or reducing the impact of the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15b93d-eeae-4333-8712-8dc6a5e99adc",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "ans->i)Identify the range for each feature:\n",
    "\n",
    "Price: Determine the minimum and maximum possible prices for food items.\n",
    "\n",
    "Rating: Typically, ratings range from 1 to 5.\n",
    "\n",
    "Delivery time: Determine the minimum and maximum expected delivery times.\n",
    "\n",
    "ii)Apply Min-Max scaling individually to each feature:\n",
    "\n",
    "For each feature, use the Min-Max scaling formula to transform the values into a common range of 0 to 1.\n",
    "\n",
    "This ensures that all features have the same scale and do not dominate each other during recommendation calculations.\n",
    "\n",
    "iii)After scaling, the data for each feature will be in the 0 to 1 range, making it suitable for building a recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b89a42-ddad-45e8-b30d-97864fb0d2a1",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "ans->To use PCA to reduce the dimensionality of the dataset for predicting stock prices:\n",
    "\n",
    "1.Standardize the features: Ensure all features have the same scale.\n",
    "\n",
    "2.Compute the covariance matrix of the standardized features.\n",
    "\n",
    "3.Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4.Select the top principal components based on the explained variance or a chosen threshold.\n",
    "\n",
    "5.Project the data onto these selected principal components.\n",
    "\n",
    "This reduces the dataset's dimensionality while retaining the most relevant information for stock price prediction, potentially improving model efficiency and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05353c-3839-48d8-89a6-81bd772c770f",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9283e4e9-9143-43b6-8a9d-c84c0e53763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[1, 5, 10, 15, 20]\n",
    "l2=[]\n",
    "for i in l1:\n",
    "    x= (i-min(l1))/(max(l1)-min(l1))\n",
    "    l2.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bab6ff0-7bf8-415a-b72a-f7457758b6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee8b7f-ebdb-4302-bd18-f073b70d71bf",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "ans->The number of principal components to retain in PCA for feature extraction depends on your specific goals and the explained variance you want to capture. Common approaches include retaining enough components to capture a high percentage of the variance (e.g., 95% or 99%) or using domain knowledge to select relevant components. The choice should align with your project's requirements and computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a72b3-b550-45ea-8188-72ba86ba96d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
